{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3c716bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mrjob in ./lib/python3.9/site-packages (0.7.4)\r\n",
      "Requirement already satisfied: PyYAML>=3.10 in ./lib/python3.9/site-packages (from mrjob) (6.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install mrjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc653437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"mary\" 9\n",
      "\"barlow\" 1\n",
      "\"looked\" 4\n",
      "\"at\" 4\n",
      "\"the\" 12\n",
      "\"tattered\" 3\n",
      "\"hawk\" 3\n",
      "\"in\" 4\n",
      "\"her\" 8\n",
      "\"hands\" 1\n",
      "\"and\" 9\n",
      "\"felt\" 1\n",
      "\"calm\" 2\n",
      "\"she\" 11\n",
      "\"walked\" 1\n",
      "\"over\" 1\n",
      "\"to\" 4\n",
      "\"window\" 1\n",
      "\"reflected\" 1\n",
      "\"on\" 2\n",
      "\"backward\" 1\n",
      "\"surroundings\" 1\n",
      "\"had\" 5\n",
      "\"always\" 1\n",
      "\"loved\" 1\n",
      "\"sunny\" 1\n",
      "\"plymouth\" 1\n",
      "\"with\" 5\n",
      "\"its\" 1\n",
      "\"rotten\" 1\n",
      "\"rapid\" 1\n",
      "\"rivers\" 1\n",
      "\"it\" 3\n",
      "\"was\" 5\n",
      "\"a\" 12\n",
      "\"place\" 1\n",
      "\"that\" 4\n",
      "\"encouraged\" 1\n",
      "\"tendency\" 1\n",
      "\"feel\" 1\n",
      "\"then\" 2\n",
      "\"saw\" 2\n",
      "\"something\" 1\n",
      "\"distance\" 1\n",
      "\"or\" 1\n",
      "\"rather\" 1\n",
      "\"someone\" 1\n",
      "\"figure\" 1\n",
      "\"of\" 3\n",
      "\"albert\" 10\n",
      "\"parkes\" 1\n",
      "\"grateful\" 2\n",
      "\"queen\" 1\n",
      "\"handsome\" 2\n",
      "\"ankles\" 3\n",
      "\"moist\" 2\n",
      "\"moles\" 3\n",
      "\"gulped\" 1\n",
      "\"glanced\" 1\n",
      "\"own\" 1\n",
      "\"reflection\" 1\n",
      "\"delightful\" 3\n",
      "\"greedy\" 2\n",
      "\"wine\" 2\n",
      "\"drinker\" 1\n",
      "\"ample\" 1\n",
      "\"curvy\" 1\n",
      "\"friends\" 1\n",
      "\"as\" 2\n",
      "\"weary\" 1\n",
      "\"wandering\" 1\n",
      "\"wally\" 1\n",
      "\"once\" 2\n",
      "\"even\" 3\n",
      "\"rescued\" 2\n",
      "\"an\" 2\n",
      "\"empty\" 2\n",
      "\"old\" 2\n",
      "\"man\" 2\n",
      "\"from\" 2\n",
      "\"burning\" 2\n",
      "\"building\" 2\n",
      "\"but\" 2\n",
      "\"not\" 2\n",
      "\"person\" 1\n",
      "\"who\" 1\n",
      "\"prepared\" 1\n",
      "\"for\" 2\n",
      "\"what\" 2\n",
      "\"store\" 1\n",
      "\"today\" 1\n",
      "\"clouds\" 1\n",
      "\"danced\" 1\n",
      "\"like\" 3\n",
      "\"thinking\" 1\n",
      "\"frogs\" 1\n",
      "\"making\" 1\n",
      "\"shocked\" 2\n",
      "\"stepped\" 1\n",
      "\"outside\" 1\n",
      "\"came\" 2\n",
      "\"closer\" 1\n",
      "\"could\" 1\n",
      "\"see\" 1\n",
      "\"bumpy\" 1\n",
      "\"smile\" 1\n",
      "\"his\" 2\n",
      "\"face\" 1\n",
      "\"look\" 1\n",
      "\"growled\" 1\n",
      "\"cowardly\" 1\n",
      "\"glare\" 1\n",
      "\"reminded\" 1\n",
      "\"maggots\" 1\n",
      "\"s\" 3\n",
      "\"i\" 6\n",
      "\"don\" 3\n",
      "\"t\" 3\n",
      "\"love\" 1\n",
      "\"you\" 4\n",
      "\"want\" 2\n",
      "\"wifi\" 1\n",
      "\"code\" 1\n",
      "\"owe\" 2\n",
      "\"me\" 2\n",
      "\"5827\" 1\n",
      "\"euros\" 1\n",
      "\"back\" 1\n",
      "\"more\" 1\n",
      "\"still\" 1\n",
      "\"fingering\" 1\n",
      "\"ate\" 1\n",
      "\"your\" 1\n",
      "\"puppy\" 1\n",
      "\"replied\" 1\n",
      "\"they\" 1\n",
      "\"each\" 1\n",
      "\"other\" 1\n",
      "\"surprised\" 1\n",
      "\"feelings\" 1\n",
      "\"two\" 2\n",
      "\"mangled\" 1\n",
      "\"mighty\" 1\n",
      "\"mice\" 1\n",
      "\"jogging\" 1\n",
      "\"very\" 1\n",
      "\"vile\" 1\n",
      "\"engagement\" 1\n",
      "\"party\" 1\n",
      "\"which\" 1\n",
      "\"piano\" 1\n",
      "\"music\" 1\n",
      "\"playing\" 1\n",
      "\"background\" 1\n",
      "\"selfish\" 1\n",
      "\"uncles\" 1\n",
      "\"gyrating\" 1\n",
      "\"beat\" 1\n",
      "\"regarded\" 1\n",
      "\"have\" 2\n",
      "\"funds\" 2\n",
      "\"lied\" 1\n",
      "\"glared\" 1\n",
      "\"do\" 2\n",
      "\"shove\" 1\n",
      "\"where\" 1\n",
      "\"sun\" 1\n",
      "\"shine\" 1\n",
      "\"promptly\" 1\n",
      "\"remembered\" 1\n",
      "\"values\" 1\n",
      "\"actually\" 1\n",
      "\"admitted\" 1\n",
      "\"reached\" 1\n",
      "\"into\" 1\n",
      "\"pockets\" 1\n",
      "\"here\" 1\n",
      "\"stable\" 1\n",
      "\"wallet\" 1\n",
      "\"blushing\" 1\n",
      "\"knowing\" 1\n",
      "\"knobby\" 1\n",
      "\"knife\" 1\n",
      "\"inside\" 1\n",
      "\"nice\" 1\n",
      "\"glass\" 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "# Function to read the input text file and return its content as a string\n",
    "def read_input_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Map function to tokenize the text and emit key-value pairs for each word\n",
    "def mapper(text):\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    for word in words:\n",
    "        yield (word, 1)\n",
    "\n",
    "# Reduce function to count the occurrences of each word\n",
    "def reducer(word, counts):\n",
    "    total_count = sum(counts)\n",
    "    yield (word, total_count)\n",
    "\n",
    "# Main function to run the MapReduce job\n",
    "def main(input_file_path):\n",
    "    input_text = read_input_file(input_file_path)\n",
    "    \n",
    "    # Map phase\n",
    "    mapped_data = mapper(input_text)\n",
    "    \n",
    "    # Grouping the data by keys\n",
    "    grouped_data = {}\n",
    "    for word, count in mapped_data:\n",
    "        grouped_data.setdefault(word, []).append(count)\n",
    "    \n",
    "    # Reduce phase\n",
    "    reduced_data = [result for word, counts in grouped_data.items() for result in reducer(word, counts)]\n",
    "    \n",
    "    return reduced_data\n",
    "\n",
    "# Example input file and running the main function\n",
    "input_file_path = 'story_time.txt'  \n",
    "output = main(input_file_path)\n",
    "\n",
    "# Displaying the output\n",
    "for word, count in output:\n",
    "    print(f'\"{word}\" {count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9a3fe60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"mary\" 9\n",
      "\"barlow\" 1\n",
      "\"looked\" 4\n",
      "\"at\" 4\n",
      "\"tattered\" 3\n",
      "\"hawk\" 3\n",
      "\"her\" 8\n",
      "\"hands\" 1\n",
      "\"felt\" 1\n",
      "\"calm\" 2\n",
      "\"she\" 11\n",
      "\"walked\" 1\n",
      "\"over\" 1\n",
      "\"window\" 1\n",
      "\"reflected\" 1\n",
      "\"on\" 2\n",
      "\"backward\" 1\n",
      "\"surroundings\" 1\n",
      "\"had\" 5\n",
      "\"always\" 1\n",
      "\"loved\" 1\n",
      "\"sunny\" 1\n",
      "\"plymouth\" 1\n",
      "\"with\" 5\n",
      "\"its\" 1\n",
      "\"rotten\" 1\n",
      "\"rapid\" 1\n",
      "\"rivers\" 1\n",
      "\"was\" 5\n",
      "\"place\" 1\n",
      "\"that\" 4\n",
      "\"encouraged\" 1\n",
      "\"tendency\" 1\n",
      "\"feel\" 1\n",
      "\"then\" 2\n",
      "\"saw\" 2\n",
      "\"something\" 1\n",
      "\"distance\" 1\n",
      "\"or\" 1\n",
      "\"rather\" 1\n",
      "\"someone\" 1\n",
      "\"figure\" 1\n",
      "\"albert\" 10\n",
      "\"parkes\" 1\n",
      "\"grateful\" 2\n",
      "\"queen\" 1\n",
      "\"handsome\" 2\n",
      "\"ankles\" 3\n",
      "\"moist\" 2\n",
      "\"moles\" 3\n",
      "\"gulped\" 1\n",
      "\"glanced\" 1\n",
      "\"own\" 1\n",
      "\"reflection\" 1\n",
      "\"delightful\" 3\n",
      "\"greedy\" 2\n",
      "\"wine\" 2\n",
      "\"drinker\" 1\n",
      "\"ample\" 1\n",
      "\"curvy\" 1\n",
      "\"friends\" 1\n",
      "\"as\" 2\n",
      "\"weary\" 1\n",
      "\"wandering\" 1\n",
      "\"wally\" 1\n",
      "\"once\" 2\n",
      "\"even\" 3\n",
      "\"rescued\" 2\n",
      "\"an\" 2\n",
      "\"empty\" 2\n",
      "\"old\" 2\n",
      "\"man\" 2\n",
      "\"from\" 2\n",
      "\"burning\" 2\n",
      "\"building\" 2\n",
      "\"but\" 2\n",
      "\"not\" 2\n",
      "\"person\" 1\n",
      "\"who\" 1\n",
      "\"prepared\" 1\n",
      "\"for\" 2\n",
      "\"what\" 2\n",
      "\"store\" 1\n",
      "\"today\" 1\n",
      "\"clouds\" 1\n",
      "\"danced\" 1\n",
      "\"like\" 3\n",
      "\"thinking\" 1\n",
      "\"frogs\" 1\n",
      "\"making\" 1\n",
      "\"shocked\" 2\n",
      "\"stepped\" 1\n",
      "\"outside\" 1\n",
      "\"came\" 2\n",
      "\"closer\" 1\n",
      "\"could\" 1\n",
      "\"see\" 1\n",
      "\"bumpy\" 1\n",
      "\"smile\" 1\n",
      "\"his\" 2\n",
      "\"face\" 1\n",
      "\"look\" 1\n",
      "\"growled\" 1\n",
      "\"cowardly\" 1\n",
      "\"glare\" 1\n",
      "\"reminded\" 1\n",
      "\"maggots\" 1\n",
      "\"s\" 3\n",
      "\"i\" 6\n",
      "\"don\" 3\n",
      "\"t\" 3\n",
      "\"love\" 1\n",
      "\"you\" 4\n",
      "\"want\" 2\n",
      "\"wifi\" 1\n",
      "\"code\" 1\n",
      "\"owe\" 2\n",
      "\"me\" 2\n",
      "\"5827\" 1\n",
      "\"euros\" 1\n",
      "\"back\" 1\n",
      "\"more\" 1\n",
      "\"still\" 1\n",
      "\"fingering\" 1\n",
      "\"ate\" 1\n",
      "\"your\" 1\n",
      "\"puppy\" 1\n",
      "\"replied\" 1\n",
      "\"they\" 1\n",
      "\"each\" 1\n",
      "\"other\" 1\n",
      "\"surprised\" 1\n",
      "\"feelings\" 1\n",
      "\"two\" 2\n",
      "\"mangled\" 1\n",
      "\"mighty\" 1\n",
      "\"mice\" 1\n",
      "\"jogging\" 1\n",
      "\"very\" 1\n",
      "\"vile\" 1\n",
      "\"engagement\" 1\n",
      "\"party\" 1\n",
      "\"which\" 1\n",
      "\"piano\" 1\n",
      "\"music\" 1\n",
      "\"playing\" 1\n",
      "\"background\" 1\n",
      "\"selfish\" 1\n",
      "\"uncles\" 1\n",
      "\"gyrating\" 1\n",
      "\"beat\" 1\n",
      "\"regarded\" 1\n",
      "\"have\" 2\n",
      "\"funds\" 2\n",
      "\"lied\" 1\n",
      "\"glared\" 1\n",
      "\"do\" 2\n",
      "\"shove\" 1\n",
      "\"where\" 1\n",
      "\"sun\" 1\n",
      "\"shine\" 1\n",
      "\"promptly\" 1\n",
      "\"remembered\" 1\n",
      "\"values\" 1\n",
      "\"actually\" 1\n",
      "\"admitted\" 1\n",
      "\"reached\" 1\n",
      "\"into\" 1\n",
      "\"pockets\" 1\n",
      "\"here\" 1\n",
      "\"stable\" 1\n",
      "\"wallet\" 1\n",
      "\"blushing\" 1\n",
      "\"knowing\" 1\n",
      "\"knobby\" 1\n",
      "\"knife\" 1\n",
      "\"inside\" 1\n",
      "\"nice\" 1\n",
      "\"glass\" 1\n"
     ]
    }
   ],
   "source": [
    "# List of stopwords\n",
    "stopwords = ['the', 'and', 'of', 'a', 'to', 'in', 'is', 'it']\n",
    "\n",
    "# Function to read the input text file and return its content as a string\n",
    "def read_input_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Map function to tokenize the text and emit key-value pairs for each non-stop word\n",
    "def mapper(text):\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            yield (word, 1)\n",
    "\n",
    "# Reduce function to count the occurrences of each non-stop word\n",
    "def reducer(word, counts):\n",
    "    total_count = sum(counts)\n",
    "    yield (word, total_count)\n",
    "\n",
    "# Main function to run the MapReduce job\n",
    "def main(input_file_path):\n",
    "    input_text = read_input_file(input_file_path)\n",
    "    \n",
    "    # Map phase\n",
    "    mapped_data = mapper(input_text)\n",
    "    \n",
    "    # Grouping the data by keys\n",
    "    grouped_data = {}\n",
    "    for word, count in mapped_data:\n",
    "        grouped_data.setdefault(word, []).append(count)\n",
    "    \n",
    "    # Reduce phase\n",
    "    reduced_data = [result for word, counts in grouped_data.items() for result in reducer(word, counts)]\n",
    "    \n",
    "    return reduced_data\n",
    "\n",
    "# Example input file and running the main function\n",
    "input_file_path = 'story_time.txt'  \n",
    "output = main(input_file_path)\n",
    "\n",
    "# Displaying the output\n",
    "for word, count in output:\n",
    "    print(f'\"{word}\" {count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03c63ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"mary,barlow\" 1\n",
      "\"barlow,looked\" 1\n",
      "\"looked,at\" 2\n",
      "\"at,the\" 1\n",
      "\"the,tattered\" 2\n",
      "\"tattered,hawk\" 3\n",
      "\"hawk,in\" 1\n",
      "\"in,her\" 1\n",
      "\"her,hands\" 1\n",
      "\"hands,and\" 1\n",
      "\"and,felt\" 1\n",
      "\"felt,calm\" 1\n",
      "\"she,walked\" 1\n",
      "\"walked,over\" 1\n",
      "\"over,to\" 1\n",
      "\"to,the\" 2\n",
      "\"the,window\" 1\n",
      "\"window,and\" 1\n",
      "\"and,reflected\" 1\n",
      "\"reflected,on\" 1\n",
      "\"on,her\" 1\n",
      "\"her,backward\" 1\n",
      "\"backward,surroundings\" 1\n",
      "\"surroundings,she\" 1\n",
      "\"she,had\" 2\n",
      "\"had,always\" 1\n",
      "\"always,loved\" 1\n",
      "\"loved,sunny\" 1\n",
      "\"sunny,plymouth\" 1\n",
      "\"plymouth,with\" 1\n",
      "\"with,its\" 1\n",
      "\"its,rotten\" 1\n",
      "\"rotten,rapid\" 1\n",
      "\"rapid,rivers\" 1\n",
      "\"rivers,it\" 1\n",
      "\"it,was\" 2\n",
      "\"was,a\" 3\n",
      "\"a,place\" 1\n",
      "\"place,that\" 1\n",
      "\"that,encouraged\" 1\n",
      "\"encouraged,her\" 1\n",
      "\"her,tendency\" 1\n",
      "\"tendency,to\" 1\n",
      "\"to,feel\" 1\n",
      "\"feel,calm\" 1\n",
      "\"then,she\" 1\n",
      "\"she,saw\" 1\n",
      "\"saw,something\" 1\n",
      "\"something,in\" 1\n",
      "\"in,the\" 2\n",
      "\"the,distance\" 1\n",
      "\"distance,or\" 1\n",
      "\"or,rather\" 1\n",
      "\"rather,someone\" 1\n",
      "\"someone,it\" 1\n",
      "\"was,the\" 1\n",
      "\"the,figure\" 1\n",
      "\"figure,of\" 1\n",
      "\"of,albert\" 1\n",
      "\"albert,parkes\" 1\n",
      "\"parkes,albert\" 1\n",
      "\"albert,was\" 1\n",
      "\"a,grateful\" 1\n",
      "\"grateful,queen\" 1\n",
      "\"queen,with\" 1\n",
      "\"with,handsome\" 1\n",
      "\"handsome,ankles\" 2\n",
      "\"ankles,and\" 3\n",
      "\"and,moist\" 2\n",
      "\"moist,moles\" 2\n",
      "\"mary,gulped\" 1\n",
      "\"gulped,she\" 1\n",
      "\"she,glanced\" 1\n",
      "\"glanced,at\" 1\n",
      "\"at,her\" 1\n",
      "\"her,own\" 1\n",
      "\"own,reflection\" 1\n",
      "\"reflection,she\" 1\n",
      "\"she,was\" 1\n",
      "\"a,delightful\" 2\n",
      "\"delightful,greedy\" 1\n",
      "\"greedy,wine\" 1\n",
      "\"wine,drinker\" 1\n",
      "\"drinker,with\" 1\n",
      "\"with,ample\" 1\n",
      "\"ample,ankles\" 1\n",
      "\"and,curvy\" 1\n",
      "\"curvy,moles\" 1\n",
      "\"moles,her\" 1\n",
      "\"her,friends\" 1\n",
      "\"friends,saw\" 1\n",
      "\"saw,her\" 1\n",
      "\"her,as\" 1\n",
      "\"as,a\" 1\n",
      "\"a,weary\" 1\n",
      "\"weary,wandering\" 1\n",
      "\"wandering,wally\" 1\n",
      "\"wally,once\" 1\n",
      "\"once,she\" 1\n",
      "\"had,even\" 1\n",
      "\"even,rescued\" 1\n",
      "\"rescued,an\" 2\n",
      "\"an,empty\" 2\n",
      "\"empty,old\" 2\n",
      "\"old,man\" 2\n",
      "\"man,from\" 2\n",
      "\"from,a\" 2\n",
      "\"a,burning\" 2\n",
      "\"burning,building\" 2\n",
      "\"but,not\" 1\n",
      "\"not,even\" 1\n",
      "\"even,a\" 1\n",
      "\"delightful,person\" 1\n",
      "\"person,who\" 1\n",
      "\"who,had\" 1\n",
      "\"had,once\" 1\n",
      "\"once,rescued\" 1\n",
      "\"building,was\" 1\n",
      "\"was,prepared\" 1\n",
      "\"prepared,for\" 1\n",
      "\"for,what\" 1\n",
      "\"what,albert\" 1\n",
      "\"albert,had\" 1\n",
      "\"had,in\" 1\n",
      "\"in,store\" 1\n",
      "\"store,today\" 1\n",
      "\"the,clouds\" 1\n",
      "\"clouds,danced\" 1\n",
      "\"danced,like\" 1\n",
      "\"like,thinking\" 1\n",
      "\"thinking,frogs\" 1\n",
      "\"frogs,making\" 1\n",
      "\"making,mary\" 1\n",
      "\"mary,shocked\" 1\n",
      "\"as,mary\" 1\n",
      "\"mary,stepped\" 1\n",
      "\"stepped,outside\" 1\n",
      "\"outside,and\" 1\n",
      "\"and,albert\" 1\n",
      "\"albert,came\" 2\n",
      "\"came,closer\" 1\n",
      "\"closer,she\" 1\n",
      "\"she,could\" 1\n",
      "\"could,see\" 1\n",
      "\"see,the\" 1\n",
      "\"the,bumpy\" 1\n",
      "\"bumpy,smile\" 1\n",
      "\"smile,on\" 1\n",
      "\"on,his\" 1\n",
      "\"his,face\" 1\n",
      "\"look,mary\" 1\n",
      "\"mary,growled\" 1\n",
      "\"growled,albert\" 1\n",
      "\"albert,with\" 1\n",
      "\"with,a\" 1\n",
      "\"a,cowardly\" 1\n",
      "\"cowardly,glare\" 1\n",
      "\"glare,that\" 1\n",
      "\"that,reminded\" 1\n",
      "\"reminded,mary\" 1\n",
      "\"mary,of\" 1\n",
      "\"of,grateful\" 1\n",
      "\"grateful,maggots\" 1\n",
      "\"maggots,it\" 1\n",
      "\"it,s\" 1\n",
      "\"s,not\" 1\n",
      "\"not,that\" 1\n",
      "\"that,i\" 1\n",
      "\"i,don\" 2\n",
      "\"don,t\" 3\n",
      "\"t,love\" 1\n",
      "\"love,you\" 1\n",
      "\"you,but\" 1\n",
      "\"but,i\" 1\n",
      "\"i,want\" 1\n",
      "\"want,a\" 1\n",
      "\"a,wifi\" 1\n",
      "\"wifi,code\" 1\n",
      "\"code,you\" 1\n",
      "\"you,owe\" 1\n",
      "\"owe,me\" 1\n",
      "\"me,5827\" 1\n",
      "\"5827,euros\" 1\n",
      "\"mary,looked\" 1\n",
      "\"looked,back\" 1\n",
      "\"back,even\" 1\n",
      "\"even,more\" 1\n",
      "\"more,shocked\" 1\n",
      "\"shocked,and\" 1\n",
      "\"and,still\" 1\n",
      "\"still,fingering\" 1\n",
      "\"fingering,the\" 1\n",
      "\"hawk,albert\" 1\n",
      "\"albert,i\" 1\n",
      "\"i,ate\" 1\n",
      "\"ate,your\" 1\n",
      "\"your,puppy\" 1\n",
      "\"puppy,she\" 1\n",
      "\"she,replied\" 1\n",
      "\"they,looked\" 1\n",
      "\"at,each\" 1\n",
      "\"each,other\" 1\n",
      "\"other,with\" 1\n",
      "\"with,surprised\" 1\n",
      "\"surprised,feelings\" 1\n",
      "\"feelings,like\" 1\n",
      "\"like,two\" 1\n",
      "\"two,mangled\" 1\n",
      "\"mangled,mighty\" 1\n",
      "\"mighty,mice\" 1\n",
      "\"mice,jogging\" 1\n",
      "\"jogging,at\" 1\n",
      "\"at,a\" 1\n",
      "\"a,very\" 1\n",
      "\"very,vile\" 1\n",
      "\"vile,engagement\" 1\n",
      "\"engagement,party\" 1\n",
      "\"party,which\" 1\n",
      "\"which,had\" 1\n",
      "\"had,piano\" 1\n",
      "\"piano,music\" 1\n",
      "\"music,playing\" 1\n",
      "\"playing,in\" 1\n",
      "\"the,background\" 1\n",
      "\"background,and\" 1\n",
      "\"and,two\" 1\n",
      "\"two,selfish\" 1\n",
      "\"selfish,uncles\" 1\n",
      "\"uncles,gyrating\" 1\n",
      "\"gyrating,to\" 1\n",
      "\"the,beat\" 1\n",
      "\"mary,regarded\" 1\n",
      "\"regarded,albert\" 1\n",
      "\"albert,s\" 1\n",
      "\"s,handsome\" 1\n",
      "\"moles,i\" 1\n",
      "\"t,have\" 1\n",
      "\"have,the\" 2\n",
      "\"the,funds\" 2\n",
      "\"funds,she\" 2\n",
      "\"she,lied\" 1\n",
      "\"albert,glared\" 1\n",
      "\"glared,do\" 1\n",
      "\"do,you\" 1\n",
      "\"you,want\" 1\n",
      "\"want,me\" 1\n",
      "\"me,to\" 1\n",
      "\"to,shove\" 1\n",
      "\"shove,that\" 1\n",
      "\"that,tattered\" 1\n",
      "\"hawk,where\" 1\n",
      "\"where,the\" 1\n",
      "\"the,sun\" 1\n",
      "\"sun,don\" 1\n",
      "\"t,shine\" 1\n",
      "\"mary,promptly\" 1\n",
      "\"promptly,remembered\" 1\n",
      "\"remembered,her\" 1\n",
      "\"her,delightful\" 1\n",
      "\"delightful,and\" 1\n",
      "\"and,greedy\" 1\n",
      "\"greedy,values\" 1\n",
      "\"values,actually\" 1\n",
      "\"actually,i\" 1\n",
      "\"i,do\" 1\n",
      "\"do,have\" 1\n",
      "\"she,admitted\" 1\n",
      "\"admitted,she\" 1\n",
      "\"she,reached\" 1\n",
      "\"reached,into\" 1\n",
      "\"into,her\" 1\n",
      "\"her,pockets\" 1\n",
      "\"pockets,here\" 1\n",
      "\"here,s\" 1\n",
      "\"s,what\" 1\n",
      "\"what,i\" 1\n",
      "\"i,owe\" 1\n",
      "\"owe,you\" 1\n",
      "\"albert,looked\" 1\n",
      "\"looked,stable\" 1\n",
      "\"stable,his\" 1\n",
      "\"his,wallet\" 1\n",
      "\"wallet,blushing\" 1\n",
      "\"blushing,like\" 1\n",
      "\"like,a\" 1\n",
      "\"a,knowing\" 1\n",
      "\"knowing,knobby\" 1\n",
      "\"knobby,knife\" 1\n",
      "\"then,albert\" 1\n",
      "\"came,inside\" 1\n",
      "\"inside,for\" 1\n",
      "\"for,a\" 1\n",
      "\"a,nice\" 1\n",
      "\"nice,glass\" 1\n",
      "\"glass,of\" 1\n",
      "\"of,wine\" 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "# Function to read the input text file and return its content as a string\n",
    "def read_input_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Map function to tokenize the text and emit word bigrams as key-value pairs\n",
    "def mapper(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        words = re.findall(r'\\w+', line.lower())\n",
    "        for i in range(len(words) - 1):\n",
    "            bigram = f'{words[i]},{words[i + 1]}'\n",
    "            yield (bigram, 1)\n",
    "\n",
    "# Reduce function to count the occurrences of each word bigram\n",
    "def reducer(bigram, counts):\n",
    "    total_count = sum(counts)\n",
    "    yield (bigram, total_count)\n",
    "\n",
    "# Main function to run the MapReduce job\n",
    "def main(input_file_path):\n",
    "    input_text = read_input_file(input_file_path)\n",
    "    \n",
    "    # Map phase\n",
    "    mapped_data = mapper(input_text)\n",
    "    \n",
    "    # Grouping the data by keys\n",
    "    grouped_data = {}\n",
    "    for bigram, count in mapped_data:\n",
    "        grouped_data.setdefault(bigram, []).append(count)\n",
    "    \n",
    "    # Reduce phase\n",
    "    reduced_data = [result for bigram, counts in grouped_data.items() for result in reducer(bigram, counts)]\n",
    "    \n",
    "    return reduced_data\n",
    "\n",
    "# Example input file and running the main function\n",
    "input_file_path = 'story_time.txt'  \n",
    "output = main(input_file_path)\n",
    "\n",
    "# Displaying the output\n",
    "for bigram, count in output:\n",
    "    print(f'\"{bigram}\" {count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91f98c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"mary\" Document 1, Document 15, Document 21, Document 17, Document 13, Document 25, Document 7, Document 11\n",
      "\"barlow\" Document 1\n",
      "\"looked\" Document 1, Document 27, Document 17, Document 19\n",
      "\"at\" Document 1, Document 7, Document 19\n",
      "\"the\" Document 1, Document 3, Document 21, Document 17, Document 23, Document 5, Document 19, Document 13, Document 25, Document 11\n",
      "\"tattered\" Document 1, Document 17, Document 23\n",
      "\"hawk\" Document 1, Document 17, Document 23\n",
      "\"in\" Document 5, Document 1, Document 19, Document 9\n",
      "\"her\" Document 1, Document 25, Document 7, Document 3\n",
      "\"hands\" Document 1\n",
      "\"and\" Document 1, Document 3, Document 21, Document 17, Document 5, Document 19, Document 13, Document 25, Document 7\n",
      "\"felt\" Document 1\n",
      "\"calm\" Document 1, Document 3\n",
      "\"she\" Document 3, Document 21, Document 17, Document 5, Document 13, Document 25, Document 7\n",
      "\"walked\" Document 3\n",
      "\"over\" Document 3\n",
      "\"to\" Document 23, Document 3, Document 19\n",
      "\"window\" Document 3\n",
      "\"reflected\" Document 3\n",
      "\"on\" Document 13, Document 3\n",
      "\"backward\" Document 3\n",
      "\"surroundings\" Document 3\n",
      "\"had\" Document 7, Document 3, Document 19, Document 9\n",
      "\"always\" Document 3\n",
      "\"loved\" Document 3\n",
      "\"sunny\" Document 3\n",
      "\"plymouth\" Document 3\n",
      "\"with\" Document 15, Document 3, Document 5, Document 19, Document 7\n",
      "\"its\" Document 3\n",
      "\"rotten\" Document 3\n",
      "\"rapid\" Document 3\n",
      "\"rivers\" Document 3\n",
      "\"it\" Document 5, Document 15, Document 3\n",
      "\"was\" Document 5, Document 7, Document 3, Document 9\n",
      "\"a\" Document 29, Document 15, Document 27, Document 3, Document 9, Document 5, Document 19, Document 7\n",
      "\"place\" Document 3\n",
      "\"that\" Document 15, Document 23, Document 3\n",
      "\"encouraged\" Document 3\n",
      "\"tendency\" Document 3\n",
      "\"feel\" Document 3\n",
      "\"then\" Document 5, Document 29\n",
      "\"saw\" Document 5, Document 7\n",
      "\"something\" Document 5\n",
      "\"distance\" Document 5\n",
      "\"or\" Document 5\n",
      "\"rather\" Document 5\n",
      "\"someone\" Document 5\n",
      "\"figure\" Document 5\n",
      "\"of\" Document 5, Document 15, Document 29\n",
      "\"albert\" Document 29, Document 15, Document 27, Document 21, Document 17, Document 23, Document 9, Document 5, Document 13\n",
      "\"parkes\" Document 5\n",
      "\"grateful\" Document 5, Document 15\n",
      "\"queen\" Document 5\n",
      "\"handsome\" Document 5, Document 21\n",
      "\"ankles\" Document 5, Document 21, Document 7\n",
      "\"moist\" Document 5, Document 21\n",
      "\"moles\" Document 5, Document 21, Document 7\n",
      "\"gulped\" Document 7\n",
      "\"glanced\" Document 7\n",
      "\"own\" Document 7\n",
      "\"reflection\" Document 7\n",
      "\"delightful\" Document 25, Document 7, Document 9\n",
      "\"greedy\" Document 25, Document 7\n",
      "\"wine\" Document 29, Document 7\n",
      "\"drinker\" Document 7\n",
      "\"ample\" Document 7\n",
      "\"curvy\" Document 7\n",
      "\"friends\" Document 7\n",
      "\"as\" Document 13, Document 7\n",
      "\"weary\" Document 7\n",
      "\"wandering\" Document 7\n",
      "\"wally\" Document 7\n",
      "\"once\" Document 7, Document 9\n",
      "\"even\" Document 17, Document 7, Document 9\n",
      "\"rescued\" Document 7, Document 9\n",
      "\"an\" Document 7, Document 9\n",
      "\"empty\" Document 7, Document 9\n",
      "\"old\" Document 7, Document 9\n",
      "\"man\" Document 7, Document 9\n",
      "\"from\" Document 7, Document 9\n",
      "\"burning\" Document 7, Document 9\n",
      "\"building\" Document 7, Document 9\n",
      "\"but\" Document 15, Document 9\n",
      "\"not\" Document 15, Document 9\n",
      "\"person\" Document 9\n",
      "\"who\" Document 9\n",
      "\"prepared\" Document 9\n",
      "\"for\" Document 29, Document 9\n",
      "\"what\" Document 25, Document 9\n",
      "\"store\" Document 9\n",
      "\"today\" Document 9\n",
      "\"clouds\" Document 11\n",
      "\"danced\" Document 11\n",
      "\"like\" Document 27, Document 19, Document 11\n",
      "\"thinking\" Document 11\n",
      "\"frogs\" Document 11\n",
      "\"making\" Document 11\n",
      "\"shocked\" Document 17, Document 11\n",
      "\"stepped\" Document 13\n",
      "\"outside\" Document 13\n",
      "\"came\" Document 29, Document 13\n",
      "\"closer\" Document 13\n",
      "\"could\" Document 13\n",
      "\"see\" Document 13\n",
      "\"bumpy\" Document 13\n",
      "\"smile\" Document 13\n",
      "\"his\" Document 13, Document 27\n",
      "\"face\" Document 13\n",
      "\"look\" Document 15\n",
      "\"growled\" Document 15\n",
      "\"cowardly\" Document 15\n",
      "\"glare\" Document 15\n",
      "\"reminded\" Document 15\n",
      "\"maggots\" Document 15\n",
      "\"s\" Document 15, Document 25, Document 21\n",
      "\"i\" Document 15, Document 25, Document 17, Document 21\n",
      "\"don\" Document 15, Document 21, Document 23\n",
      "\"t\" Document 15, Document 21, Document 23\n",
      "\"love\" Document 15\n",
      "\"you\" Document 15, Document 25, Document 23\n",
      "\"want\" Document 15, Document 23\n",
      "\"wifi\" Document 15\n",
      "\"code\" Document 15\n",
      "\"owe\" Document 15, Document 25\n",
      "\"me\" Document 15, Document 23\n",
      "\"5827\" Document 15\n",
      "\"euros\" Document 15\n",
      "\"back\" Document 17\n",
      "\"more\" Document 17\n",
      "\"still\" Document 17\n",
      "\"fingering\" Document 17\n",
      "\"ate\" Document 17\n",
      "\"your\" Document 17\n",
      "\"puppy\" Document 17\n",
      "\"replied\" Document 17\n",
      "\"they\" Document 19\n",
      "\"each\" Document 19\n",
      "\"other\" Document 19\n",
      "\"surprised\" Document 19\n",
      "\"feelings\" Document 19\n",
      "\"two\" Document 19\n",
      "\"mangled\" Document 19\n",
      "\"mighty\" Document 19\n",
      "\"mice\" Document 19\n",
      "\"jogging\" Document 19\n",
      "\"very\" Document 19\n",
      "\"vile\" Document 19\n",
      "\"engagement\" Document 19\n",
      "\"party\" Document 19\n",
      "\"which\" Document 19\n",
      "\"piano\" Document 19\n",
      "\"music\" Document 19\n",
      "\"playing\" Document 19\n",
      "\"background\" Document 19\n",
      "\"selfish\" Document 19\n",
      "\"uncles\" Document 19\n",
      "\"gyrating\" Document 19\n",
      "\"beat\" Document 19\n",
      "\"regarded\" Document 21\n",
      "\"have\" Document 21, Document 25\n",
      "\"funds\" Document 21, Document 25\n",
      "\"lied\" Document 21\n",
      "\"glared\" Document 23\n",
      "\"do\" Document 25, Document 23\n",
      "\"shove\" Document 23\n",
      "\"where\" Document 23\n",
      "\"sun\" Document 23\n",
      "\"shine\" Document 23\n",
      "\"promptly\" Document 25\n",
      "\"remembered\" Document 25\n",
      "\"values\" Document 25\n",
      "\"actually\" Document 25\n",
      "\"admitted\" Document 25\n",
      "\"reached\" Document 25\n",
      "\"into\" Document 25\n",
      "\"pockets\" Document 25\n",
      "\"here\" Document 25\n",
      "\"stable\" Document 27\n",
      "\"wallet\" Document 27\n",
      "\"blushing\" Document 27\n",
      "\"knowing\" Document 27\n",
      "\"knobby\" Document 27\n",
      "\"knife\" Document 27\n",
      "\"inside\" Document 29\n",
      "\"nice\" Document 29\n",
      "\"glass\" Document 29\n"
     ]
    }
   ],
   "source": [
    "# Function to read the input text file and return its content as a list of documents\n",
    "def read_input_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.readlines()\n",
    "\n",
    "# Map function to tokenize each document and emit word-documentID pairs\n",
    "def mapper(doc_id, text):\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    for word in words:\n",
    "        yield (word, doc_id)\n",
    "\n",
    "# Reduce function to group data by word and concatenate the document IDs\n",
    "def reducer(word, doc_ids):\n",
    "    doc_ids_list = list(set(doc_ids))  # Removing duplicate document IDs\n",
    "    yield (word, doc_ids_list)\n",
    "\n",
    "# Main function to run the MapReduce job\n",
    "def main(input_file_path):\n",
    "    input_docs = read_input_file(input_file_path)\n",
    "    documents = [(f'Document {i+1}', doc.strip()) for i, doc in enumerate(input_docs)]\n",
    "    \n",
    "    # Map phase\n",
    "    mapped_data = [result for doc_id, doc_text in documents for result in mapper(doc_id, doc_text)]\n",
    "    \n",
    "    # Grouping the data by keys (words)\n",
    "    grouped_data = {}\n",
    "    for word, doc_id in mapped_data:\n",
    "        grouped_data.setdefault(word, []).append(doc_id)\n",
    "    \n",
    "    # Reduce phase\n",
    "    reduced_data = [result for word, doc_ids in grouped_data.items() for result in reducer(word, doc_ids)]\n",
    "    \n",
    "    return reduced_data\n",
    "\n",
    "# Example input file and running the main function\n",
    "input_file_path = 'story_time.txt'  \n",
    "output = main(input_file_path)\n",
    "\n",
    "# Displaying the output\n",
    "for word, doc_ids in output:\n",
    "    formatted_doc_ids = \", \".join(doc_ids)\n",
    "    print(f'\"{word}\" {formatted_doc_ids}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff01275",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
